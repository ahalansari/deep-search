# Docker Swarm Deployment for DeepSearch (Portainer)
# 
# IMPORTANT: Create network and build image first, then deploy this stack
# 
# STEP 1: Create the overlay network (run on manager node):
#   docker network create --driver overlay --attachable deepsearch-network
# 
# STEP 2: Build and push the image manually:
#   docker build -t aalansari/deep-search:latest .
#   docker push aalansari/deep-search:latest
# 
# STEP 3: Deploy this stack in Portainer Swarm mode
# 
# The image aalansari/deep-search:latest will be pulled from Docker Hub
#
version: '3.8'

services:
  # Next.js DeepSearch Application  
  deepsearch-app:
    image: aalansari/deep-search:latest
    ports:
      - target: 3002
        published: 3002
        mode: host
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=${APP_URL:-http://localhost:3002}
      - SEARXNG_URL=http://searxng:8080
      - AI_URL=${AI_URL:-http://host.docker.internal:1234}
    networks:
      - deepsearch-network
    volumes:
      - app-logs:/app/logs
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3002/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 1G
          cpus: "1"
        reservations:
          memory: 512M
          cpus: "0.5"
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.deepsearch.rule=Host(`${APP_DOMAIN:-deepsearch.local}`)"
        - "traefik.http.services.deepsearch.loadbalancer.server.port=3002"
        - "com.deepsearch.service=app"

  # SearXNG Search Engine
  searxng:
    image: searxng/searxng:latest
    ports:
      - target: 8080
        published: 8080
        mode: host
    volumes:
      - searxng-settings:/etc/searxng:rw
      - searxng-data:/var/lib/searxng
    environment:
      - BASE_URL=${SEARXNG_BASE_URL:-http://localhost:8080/}
      - INSTANCE_NAME=${SEARXNG_INSTANCE_NAME:-DeepSearch-SearXNG}
      - SEARXNG_SECRET=${SEARXNG_SECRET:-please-change-this-secret-key}
    networks:
      - deepsearch-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/search?q=test&format=json"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
        reservations:
          memory: 256M
          cpus: "0.25"
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.searxng.rule=Host(`${SEARXNG_DOMAIN:-search.local}`)"
        - "traefik.http.services.searxng.loadbalancer.server.port=8080"
        - "com.deepsearch.service=searxng"

  # Redis Cache
  redis:
    image: redis:7-alpine
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory ${REDIS_MAX_MEMORY:-256mb}
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis-data:/data
    networks:
      - deepsearch-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.1"
      labels:
        - "com.deepsearch.service=redis"

  # Nginx Reverse Proxy (Optional)
  nginx:
    image: nginx:alpine
    ports:
      - target: 80
        published: 80
        mode: host
      - target: 443
        published: 443
        mode: host
    volumes:
      - nginx-config:/etc/nginx/conf.d
      - nginx-logs:/var/log/nginx
      - ssl-certs:/etc/nginx/ssl:ro
    networks:
      - deepsearch-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 64M
          cpus: "0.1"
      labels:
        - "com.deepsearch.service=proxy"

networks:
  deepsearch-network:
    external: true
    name: deepsearch-network

volumes:
  searxng-data:
  searxng-settings:
  redis-data:
  app-logs:
  nginx-config:
  nginx-logs:
  ssl-certs: